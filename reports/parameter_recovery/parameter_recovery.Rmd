---
title: "Importance sampling posteriors for CTDS processes"
author: Josh
date: 1 April 2021
output: 
  pdf_document: 
    toc: no
header-includes:
  - \usepackage{booktabs}
  - \newcommand{\bs}{\boldsymbol s}
  - \newcommand{\bg}{\boldsymbol g}
  - \newcommand{\btau}{\boldsymbol \tau}
  - \newcommand{\btheta}{\boldsymbol \theta}
  - \usepackage{natbib}
  - \bibliographystyle{abbrvnat}
params:
    tgt_dir: nim_fit
    tgt_dir_val: nim_fit_val
---

# Method

I will very briefly outline the importance sampling scheme I propose for approximating CTDS posterior distributions.  Let $\bs=(s(t_1),\dots,s(t_n))$ be a vector of observations of a CTDS trajectory at timepoints $t_1<t_2<\dots<t_n$. For estimation, we use the observation times to induce a collection of latent, sufficient statistics that partitions the complete CTDS trajectory $s(\cdot)$.  Let $g_i$ denote the path along the discrete spatial domain traversed by $s(\cdot)$ between $t_i$ and $t_{i+1}$, and let $\tau_i$ be the times at which the cell transitions in $g_i$ occur.  The full sequence of path segments $\bg=(g_1,\dots,g_n)$ and transition time vectors $\btau = (\tau_1,\dots,\tau_n)$ exactly identifies the latent trajectory $s(\cdot)$, so we use the pair $(\bg,\btau)$ to represent the trajectory during estimation.  Let $\btheta$ denote a vector of model parameters that, for example, control transition rates and directional preferences.  

I express the posterior distribution of model parameters given data $[\btheta\vert\bs]$ via the marginal distribution 
\begin{align}
\begin{split}
\label{eq:posterior_form}
  [\btheta \vert \bs] 
    &\propto \int [\bs\vert\bg, \btau] [\btheta,\btau\vert\bg] [\bg] d(\bg, \btau),
\end{split}
\end{align}
which integrates over the complete trajectory $(\bg,\btau)$.  The distribution's form is chosen such that each of its components are easy to work with computationally.  The observation component $[\bs\vert\bg, \btau]$ is degenerate in the absence of measurement error, and only serves to restrict the integration to trajectories $(\bg,\btau)$ that are consistent with observations $\bs$.  But, we can also use importance sampling to at least approximately account for measurement error.  I will not discuss this, or other alternatives, here.

The conditional joint posterior $[\btheta,\btau\vert\bg]$ is simple to approximate since all likelihood components have computationally tractable closed form expressions, and relatively simple full conditional posterior distributions.  I propose approximating the joint conditional posterior $[\btheta,\btau\vert\bg]$ via MCMC methods.  The parameter vector $\btheta$ can be updated using standard, adaptive random walk Metropolis steps, and the transition times $\btau$ can be jointly sampled by exploiting connections between continuous time Markov processes and Poisson processes.  Specifically, the transition times $\btau$ can be interpreted as conditional arrival times for a Poisson process.

The main challenge is in marginalizing out the latent path $\bg$.  I propose using a family of pre-computed path segments as the primary tool to address the issue.  Let $\mathcal G_i\subseteq \mathcal{P}^*(s_i, s_{i+1})$ be a random sample of the  shortest paths that begin at $s_i$ and end at $s_{i+1}$.  The set $\mathcal G_i$ can be sampled easily as a collection of bridged random walks, generated via forward-filtering backward-sampling algorithms.  The true path segment $g_i$ is only contained in $\mathcal{P}^*(s_i, s_{i+1})$ asymptotically, as observations become denser in time such that $\sup_i \vert t_{i+1}-t_i\vert\downarrow 0$.  So, given data, sampling from $\mathcal{P}^*(s_i, s_{i+1})$ introduces approximation error that can only be controlled by the data collection process.

I propose defining an importance sampler over $\mathcal G_1,\dots,\mathcal G_n$  to approximately marginalize out the path $\bg$ in the posterior \eqref{eq:posterior_form} via
\begin{align}
\label{eq:is_form}
  [\btheta \vert \bs]  \approx \sum_{i=1}^M [\bs\vert\bg^{(i)}, \btau] [\btheta,\btau\vert\bg^{(i)}] \frac{[\bg^{(i)}]}{f(\bg^{(i)})},
\end{align}
where $\bg^{(i)}\sim f(\bg^{(i)})$, and $\bg^{(i)}\in\mathcal G_i$.  The importance sampler draws $\bg^{(i)} = (g_1^{(i)},\dots,g_n^{(i)})$ by sampling the components of $\bg^{(i)}$ independently via
\begin{align}
  f(\bg^{(i)}) = 
  \prod_{j=1}^n 
  \frac{w_{g_j^{(i)}}}
       {\sum_{g\in\mathcal G_j} w_g  },
\end{align}
where $w_g$ is the weight of path segment $g\in\mathcal G_j$.  In our application, we weight all path segments in $\mathcal G_j$ equally.  In order to evaluate \eqref{eq:is_form}, the marginal likelihood $[\bg^{(i)}]$ must be computable as well.  We use classic MCMC results to approximate the marginal likelihood $[\bg^{(i)}]$ as the harmonic mean of the likelihood values of posterior samples \citep{newton1994}, although other more sophisticated estimators exist as well \citep[cf.][]{gelfand1994,chib1995}.


# Preliminary Results

I have simulated a trajectory for 500 time units with $\beta=0$ and $\beta_{AR}=1$, which yields a trajectory that has directional persistence (i.e., positive directional correlation) and an average of one time step between transitions (Figure \ref{fig:plot_sim}).  I look at components used to estimate the model parameters when observations are made every 5, 1, .5, and 0.25 time steps. (Aside: I have also done a simulation with $\beta_{AR}=0$, but don't show results here they are generally similar, but help highlight some of the small bias our estimation procedure has... I just don't have all the pieces in an easy-to-share format yet).

I have tested long runs of the MCMC sampler (10,000 iterations each) on 100 importance-sampled paths to assess the performance of the individual components.  The results show that parameter recovery is achieved for each importance-sampled path, where accuracy improves as the time between observations decreases (Figure \ref{fig:plot_indiv_results}).  The results also indicate standard issues with importance samplers may be an issue, in which a single particle (i.e., a single $g^{(i)}$) dominates the weighted posterior (Figure \ref{fig:plot_is_wts}).



# Discussion

We seem to have a useable method for approximating posterior distributions for CTDS models.  Using shortest paths to build the family of pre-computed path segments $\mathcal G_1,\dots,\mathcal G_n$ yields fast, accurate approximations.  While I had originally hoped to include longer paths in $\mathcal G_1,\dots,\mathcal G_n$ as well, the likelihoods for posteriors based on different path lengths appear to be practically incomparable.  I think this is due to the standard issue that it is difficult to compare likelihoods for models fit to different amounts of data (here, the number of transitions in the latent trajectory).  So, I concede that shortest-path approximations seem necessary here.

It is potentially concerning that the importance sampling weights seem to be degenerate, but we might yet remedy this issue.  Alan, did you say you had some ideas or references for how to smooth the importance weights?  Is it reasonable to truncate the importance sampling distribution or otherwise artificially compress the range of the weights (or is this what you were alluding to)?  We might not need to do too much smoothing because the weights are relatively close to each other on the log scale (Figure \ref{fig:plot_log_is_wts}).  I presume smoothing weights will bias the posterior approximation, but the reduction in variance may be well worth the tradeoff, especially since I can't imagine there is large practical variation in the imputed shortest-paths we're working with.  Furthermore, the bias is only technical since most of the individual posteriors seem to be adequately recovering model parameters (Figure \ref{fig:plot_indiv_results}).

Anyway, here, the degeneracy in the importance weights $[\bg^{(i)}]/f(\bg^{(i)})$ is driven by the difference in the marginal likelihoods $[\bg^{(1)}],\dots,[\bg^{(n)}]$ rather than the path sampling weights $f(\bg^{(1)}),\dots,f(\bg^{(n)})$.  The  path sampling weights are identical because the paths are sampled uniformly from among a sub-family of shortest paths that connect observations.  

If we cannot overcome the importance sampling weighting issue directly, then we might resort to making the same multiple-imputation argument that Hanks et al. used, and treat our overall posterior distribution as an equally-weighted mixture of posterior distributions.  But, I am not sure that this is a technically correct or helpful thing to do (and Hanks et al don't seem to justify their assertion beyond citing Rubin).  For example, weighting the posteriors equally will imply that each of the importance-sampled path imputations $\bg^{(1)},\dots,\bg^{(n)}$ is equally likely.  Or, at any rate, we won't have any posterior re-weighting of the sampled path imputations, so we won't be able to draw reliable inference on the latent trajectory.  Put differently, interpreting the paths as equally likely with respect to the posterior distribution cannot be strictly correct because these are generated by sampling bridged random walks, without regard to model parameters or covariates!

Even if we end up making appeals to Hanks et al. styles of arguments regarding multiple imputation, our posterior approximation still represents improvement.  I will work on redoing comparisons to their method.  Last time I played with this, I believe our method had faster convergence for parameter recovery.  Additionally, our method 1) avoids non-discrete AIDs, 2) automatically accounts for spatial boundary constraints, 3) allows estimation of transition times, whereas in Hanks the transition times and the path are both fixed via the AID, and 4) if we can work out the importance sample weight issues (or if they are less severe with larger samples), then we can yield CTDS model-based posterior estimates of the latent path as well.


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, cache = TRUE, 
                      dev = 'png', dpi = 300)
```

```{r load_simulation_results}
targets::tar_load(sim_fits)
targets::tar_load(sim_params)

sim_fits = readRDS('sim_fits_full.rds')
```

```{r plot_sim, fig.cap='\\label{fig:plot_sim} Coordinatewise plot of simulated trajectory with one observation overlaid at every 5 time units.', echo = FALSE}
targets::tar_load(sim_simple)
targets::tar_load(sim_obs)

par(mfrow = c(2,1))
s = stepfun(x = sim_simple$times, y = sim_simple$states[,1])
plot(s, xlab = '', ylab = 'x', do.points = FALSE, main = '')
points(sim_obs[[1]]$times, sim_obs[[1]]$states[,1])

s = stepfun(x = sim_simple$times, y = sim_simple$states[,2])
plot(s, xlab = 'Time', ylab = 'y', do.points = FALSE, main = '')
points(sim_obs[[1]]$times, sim_obs[[1]]$states[,2])

```

```{r process_results}
library(dplyr)
library(coda)

# load log_sum function
source('R/utils/log_add.R')

sim_summary_long = do.call(rbind, lapply(sim_fits, function(res) {
  
  burn = 1:10
  
  # estimate of marginal posterior
  lmp = log(length(res$samples$lp[-burn])) - log_sum(-res$samples$lp[-burn])
  
  # hpds
  hpds = HPDinterval(mcmc(res$samples$param_vec[-burn,]))
  
  data.frame(
    obs_interval = res$obs_interval,
    rep = res$rep,
    mean = (colMeans(res$samples$param_vec[-burn,])),
    var = (apply(res$samples$param_vec[-burn,], 2, var)),
    param = names(colMeans(res$samples$param_vec[-burn,])),
    log_is_wt = lmp - sum(res$samples$path$seg_ind_wts),
    lmp = lmp,
    log_path_wt = sum(res$samples$path$seg_ind_wts),
    hpds = hpds,
    truth = c(sim_params$betaAR, sim_params$beta)
  )
}))

# alternate version of summary information
sim_summary = do.call(rbind, lapply(sim_fits, function(res) {
  
  burn = 1:10
  
  # estimate of marginal posterior
  lmp = log(length(res$samples$lp[-burn])) - log_sum(-res$samples$lp[-burn])
  
  # hpds
  hpds = HPDinterval(mcmc(res$samples$param_vec[-burn,]))
  
  data.frame(
    obs_interval = res$obs_interval,
    rep = res$rep,
    mean = t(colMeans(res$samples$param_vec[-burn,])),
    var = t(apply(res$samples$param_vec[-burn,], 2, var)),
    log_is_wt = lmp - sum(res$samples$path$seg_ind_wts),
    lmp = lmp,
    log_path_wt = sum(res$samples$path$seg_ind_wts)
  )
}))

# enrich with standardized log_is_weights
sim_summary = sim_summary %>% group_by(obs_interval) %>% 
  summarise(log_is_wt_std = (log_is_wt - log_sum(log_is_wt)), rep = rep) %>% 
  ungroup() %>% left_join(sim_summary, by = c('obs_interval', 'rep'))
```


```{r plot_indiv_results, fig.cap='\\label{fig:plot_indiv_results}Posterior means and HPDs show that across each importance sample (columns), the true parameters are recovered as the time between observations decreases.'}
library(ggplot2)
library(ggthemes)
library(tidyr)
library(dplyr)

ggplot(sim_summary_long %>% filter(rep %in% 1:4), aes(x = factor(obs_interval), 
                        y = mean, ymin = hpds.lower, ymax = hpds.upper)) + 
  geom_hline(aes(yintercept = truth), lty = 3) + 
  geom_pointrange() + 
  facet_grid(param~rep, scales = 'free') + 
  xlab('Time between observations') + 
  ylab('Posterior estimate') + 
  theme_few()
```

```{r plot_is_wts, fig.cap='\\label{fig:plot_is_wts} Standardized importance sampling weight for each importance sample across all simulations.  In all cases, as is common in particle filtering for long time series, there is  a single importance sample that dominates the overall importance sample.'}
library(ggplot2)
library(ggthemes)

ggplot(sim_summary, aes(x = factor(obs_interval), y = exp(log_is_wt_std))) + 
  geom_point() + 
  ylab(expression(w[IS])) + 
    xlab('Time between observations') + 
  theme_few()
```

```{r plot_log_is_wts, fig.cap='\\label{fig:plot_log_is_wts} Log of standardized importance sampling weight for each importance sample across all simulations.  In all cases, as is common in particle filtering for long time series, there is  a single importance sample that dominates the overall importance sample.'}
library(ggplot2)
library(ggthemes)

ggplot(sim_summary, aes(x = factor(obs_interval), y = (log_is_wt_std))) + 
  geom_point() + 
  ylab(expression(log(w[IS]))) + 
    xlab('Time between observations') + 
  theme_few()
```


```{r smoothed_weights}
library(loo)
library(dplyr)

w_raw = sim_summary %>% 
    filter(obs_interval == 0.5) %>% 
    select(log_is_wt_std) %>%
    unlist()
w_smoothed = psis(log_ratios = w_raw)

w_raw_avg = log_sum(w_raw)
```

\bibliography{references}
